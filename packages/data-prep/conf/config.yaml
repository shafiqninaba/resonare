# Hydra configuration
hydra:
  run:
    dir: "."  # Run tasks in the current directory

# Input and output file paths
raw_json_filepath: ./data/raw/result.json                      # Raw Telegram export file (usually named "result.json")
processed_chats_filepath: ./data/processed/chats.json         # Output path for processed full conversations (JSON)
processed_blocks_filepath: ./data/processed/chat_blocks.jsonl  # Output path for conversation blocks (one block per line, JSONL)

# Model settings
# This is the model we're fine-tuning, so we use its tokenizer to count tokens for chunking.
# Different models have different tokenizers, so we try to use the same modelâ€™s tokenizer for accuracy.
# If the model specific tokenizer can't be loaded, we fall back to TikToken (used by OpenAI models).
model_id: "meta-llama/Llama-3.2-1B"  # HuggingFace model ID of the fine-tuning target (e.g., LLaMA)

# Chat processing options
target_name: "Ren Hwa"  # Your name in the conversation; converted to "system" in output
date_limit: None        # Optional: skip messages before this date (e.g., "2025-01-01")

# Conversation block settings
# Messages are grouped into blocks if they occur within `convo_block_thereshold_secs` of each other.
# This temporal threshold helps preserve coherent chunks of dialogue for training.
# Token limits help ensure blocks are both consistent and efficient for training: too short = weak context, too long = inefficient.
convo_block_thereshold_secs: 3600  # Max time gap (in seconds) between messages in a block
min_tokens_per_block: 300          # Discard blocks shorter than this (low context)
max_tokens_per_block: 2000         # Split or discard blocks longer than this (too large for model input)

# Message formatting
# Each message in a block is prefixed with this delimiter, simulating the flow of chat, e.g. "User": >>> Hello!"
# Note that newlines separate consecutive messages from the same speaker.
message_delimiter: ">>>"  # Prefix for each message line
