# Hydra configuration
hydra:
  run:
    dir: "."  # Run tasks in the current directory

# Input and output file paths
# - Telegram allows exporting chats either as a single JSON file (often named result.json) 
#   or as multiple JSON files, one per chat, stored in a directory.
# - Use "mode" to switch between these two formats depending on your export type.
raw_input:
  mode: "file"            # Mode to specify how chat data is loaded:
                          # - "file": Load a single JSON file (e.g., result.json).
                          # - "dir": Load multiple JSON files from a directory.
  file: "./data/raw/result.json"  # Path to the single JSON file containing all chats (e.g., Telegram export).
  dir:  "./data/raw/chats/"      # Path to the directory containing individual chat JSON files.


processed_chats_filepath: ./data/processed/chats.json         # Output path for processed full conversations (JSON)
processed_blocks_filepath: ./data/processed/chat_blocks.jsonl  # Output path for conversation blocks (one block per line, JSONL)

# Model settings
# This is the model we're fine-tuning, so we use its tokenizer to count tokens for chunking.
# Different models have different tokenizers, so we try to use the same modelâ€™s tokenizer for accuracy.
# If the model specific tokenizer can't be loaded, we fall back to TikToken (used by OpenAI models).
model_id: "meta-llama/Llama-3.2-1B"  # HuggingFace model ID of the fine-tuning target (e.g., LLaMA)

# Chat processing options
target_name: "Ren Hwa"  # Your name in the conversation; converted to "system" in output
date_limit: None        # Optional: skip messages before this date (e.g., "2025-01-01")

# Conversation block settings
# Messages are grouped into blocks if they occur within `convo_block_thereshold_secs` of each other.
# This temporal threshold helps preserve coherent chunks of dialogue for training.
# Token limits help ensure blocks are both consistent and efficient for training: too short = weak context, too long = inefficient.
convo_block_thereshold_secs: 3600  # Max time gap (in seconds) between messages in a block
min_tokens_per_block: 300          # Discard blocks shorter than this (low context)
max_tokens_per_block: 2000         # Split or discard blocks longer than this (too large for model input)

# Message formatting
# Each message in a block is prefixed with this delimiter, simulating the flow of chat, e.g. "User": >>> Hello!"
# Note that newlines separate consecutive messages from the same speaker.
message_delimiter: ">>>"  # Prefix for each message line
